{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d62a3a44",
   "metadata": {},
   "source": [
    "### Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed62e8c",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Web scraping is the process of extracting data from websites using web scrapers or crawlers. Web scraping allows users to automate the process of collecting data from the internet and convert unstructured data into structured data. \n",
    "***\n",
    "Web scraping is done for multiple reasons, for example market research, price monitoring, data analysis, and data mining.\n",
    "***\n",
    "Use Cases:\n",
    "1. E-commerce: Web scraping is commonly used in the e-commerce industry to collect product data, such as product descriptions, prices, and reviews. This data can be used to monitor competitor prices, analyze customer behavior, and optimize product listings.\n",
    "\n",
    "2. Social media: Web scraping is also commonly used in social media monitoring and sentiment analysis. It can be used to collect data on users, such as their profiles, posts, and interactions, which can be used to understand customer behavior and trends.\n",
    "\n",
    "3. Research: Web scraping is also used in academic research to collect data for studies and experiments. It can be used to collect data on a variety of topics, such as news articles, academic papers, and government data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2606ca4",
   "metadata": {},
   "source": [
    "### Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5525c4",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Web Scraping can be done in many ways, for example:\n",
    "\n",
    "1. Manual Scraping: This method involves manually copying and pasting data from websites into a spreadsheet or other data storage tool.\n",
    "\n",
    "2. Automated Scraping: This method involves using software tools called web scrapers or crawlers to automate the process of extracting data from websites. Web scrapers can be customized to collect specific data points, such as product prices, and can be programmed to scrape multiple pages at once.\n",
    "\n",
    "3. API Scraping: Many websites offer APIs (Application Programming Interfaces) that allow developers to access data from their websites directly.\n",
    "\n",
    "4. Parsing HTML: This method involves using programming languages, such as Python or PHP, to parse the HTML code of a website and extract data from specific tags and attributes.\n",
    "\n",
    "5. Browser Extension: Browser extensions, such as Chrome extensions, can be used to scrape data from websites. These extensions allow users to select specific elements on a page and extract data directly into a spreadsheet or other data storage tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d83025",
   "metadata": {},
   "source": [
    "### Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b059bbc5",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Beautiful Soup is a Python library used for web scraping purposes. It provides a simple and efficient way to parse HTML and XML documents and extract data from them.\n",
    "\n",
    "Beautiful Soup is used for several reasons, including:\n",
    "\n",
    "1. Parsing HTML: Beautiful Soup makes it easy to parse HTML documents and extract data from them, even if the HTML code is badly formed or inconsistent.\n",
    "\n",
    "2. Accessing specific elements: Beautiful Soup allows developers to access specific elements of an HTML document, such as links, images, and tables, using CSS selectors or regular expressions.\n",
    "\n",
    "3. Navigating the HTML tree: Beautiful Soup provides a way to navigate the HTML tree and access parent, sibling, and child elements of a particular tag.\n",
    "\n",
    "4. Transforming the HTML: Beautiful Soup can be used to modify the HTML code of a document by adding, removing, or modifying elements.\n",
    "\n",
    "5. Handling encoding issues: Beautiful Soup can handle encoding issues that often arise when parsing HTML documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08ed670",
   "metadata": {},
   "source": [
    "### Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ebdb82",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "Flask is used in web scraping projects for several reasons, including:\n",
    "\n",
    "1. Lightweight: Flask is a lightweight web framework that is easy to set up and configure, making it an excellent choice for small to medium-sized web scraping projects.\n",
    "\n",
    "2. Easy to use: Flask has a simple and intuitive interface that makes it easy to create web applications, APIs, and web services.\n",
    "\n",
    "3. Flexible: Flask is a flexible framework that can be customized to meet the specific needs of a web scraping project. It allows developers to choose the components they need and use only the features that are necessary.\n",
    "\n",
    "4. Integrates with other libraries: Flask integrates well with other Python libraries commonly used in web scraping, such as Beautiful Soup and requests.\n",
    "\n",
    "5. Good for prototyping: Flask is a great tool for quickly prototyping web scraping applications and testing different approaches and ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a56ca9",
   "metadata": {},
   "source": [
    "### Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd5773",
   "metadata": {},
   "source": [
    "#### Ans:\n",
    "AWS Services used are:\n",
    "1. CodePipeline\n",
    "2. Elastic Beanstalk\n",
    "***\n",
    "AWS CodePipeline: It is a fully managed continuous delivery service that helps developers automate their software release process. It allows us to create pipelines that automate the building, testing, and deployment of your code every time there is a change in your codebase.\n",
    "\n",
    "AWS Elastic Beanstalk: It is a fully-managed service that makes it easy to deploy and scale web applications and services developed with a variety of programming languages and frameworks. Elastic Beanstalk automatically handles the deployment, capacity provisioning, load balancing, and monitoring of our application. With Elastic Beanstalk, we can quickly deploy web applications and services, without having to worry about the underlying infrastructure. It supports a wide range of programming languages and frameworks, including Java, .NET, PHP, Python, Ruby, Node.js, and Docker."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
